{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba1bb23",
   "metadata": {},
   "source": [
    "## Exercise 10 â€“ Bayesian inference and Data assimilation\n",
    "\n",
    "**Group : Duckies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a198b",
   "metadata": {},
   "source": [
    "## Solution 1.1:\n",
    "\n",
    "To find the conditional density $\\pi_{X \\mid Y}(x \\mid Y=y)$, we can use Bayes' theorem, which states that:\n",
    "\n",
    "$$\n",
    "\\pi_{X \\mid Y}(x \\mid Y=y) = \\frac{\\pi_{Y \\mid X}(y \\mid x) \\pi_X(x)}{\\pi_Y(y)}\n",
    "$$\n",
    "\n",
    "where $\\pi_{Y \\mid X}(y \\mid x)$ is the likelihood function, $\\pi_X(x)$ is the prior distribution for $X$, and $\\pi_Y(y)$ is the marginal distribution for $Y$. \n",
    "\n",
    "The likelihood function can be found by considering the observation model for $Y$. Given a value of $X=x$, the distribution of $Y$ is normal with mean $h(x)=x^4+x^2$ and variance 1. Therefore, the likelihood function is:\n",
    "\n",
    "$$\n",
    "\\pi_{Y \\mid X}(y \\mid x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(y-h(x))^2}{2}}\n",
    "$$\n",
    "\n",
    "The prior distribution for $X$ is given as $N(0,2)$, so $\\pi_X(x) = \\frac{1}{\\sqrt{4\\pi}}e^{-\\frac{x^2}{4}}$.\n",
    "\n",
    "Substituting these expressions into Bayes' theorem and simplifying gives:\n",
    "\n",
    "$$\n",
    "\\pi_{X \\mid Y}(x \\mid Y=y) \\propto e^{-\\frac{(y-h(x))^2}{2}}e^{-\\frac{x^2}{4}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{X \\mid Y}(x \\mid Y=4) \\propto e^{-\\frac{(4-(x^4+x^2))^2}{2}}e^{-\\frac{x^2}{4}}\n",
    "$$\n",
    "\n",
    "This is the conditional density of $X$ given that $Y=4$, up to a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c0f5a6",
   "metadata": {},
   "source": [
    "## Solution 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1d7c7",
   "metadata": {},
   "source": [
    "To find the maximum a posteriori (MAP) estimate of $X$ given $Y=y$ using the gradient descent method, we need to minimize the negative log of the conditional density $\\pi_{X \\mid Y}(x \\mid Y=y)$. This is equivalent to maximizing the conditional density itself.\n",
    "\n",
    "Let $f(x) = -\\log(\\pi_{X \\mid Y}(x \\mid Y=y))$. Then, the gradient descent algorithm iteratively updates the estimate of $X$ using the formula:\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_{n}-\\lambda_{n} \\nabla f\\left(x_{n}\\right)\n",
    "$$\n",
    "\n",
    "where $\\lambda_{n}>0$ is a small learning rate. The algorithm stops when the magnitude of the gradient $\\nabla f$ is small enough.\n",
    "\n",
    "In this case, we have:\n",
    "\n",
    "$$\n",
    "f(x) = -\\log(\\pi_{X \\mid Y}(x \\mid Y=4)) = -\\log(e^{-\\frac{(4-(x^4+x^2))^2}{2}}e^{-\\frac{x^2}{4}}) = \\frac{(4-(x^4+x^2))^2}{2} + \\frac{x^2}{4}\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $x$, we get:\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\frac{d}{dx}f(x) = -2(4-(x^4+x^2))(4x^3+2x) + \\frac{x}{2}\n",
    "$$\n",
    "\n",
    "Substituting this expression for $\\nabla f(x)$ into the update formula for gradient descent, we can iteratively update our estimate of $X$ until convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7e9fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (4 - (x**4 + x**2))**2 / 2 + x**2 / 4\n",
    "\n",
    "def grad_f(x):\n",
    "    return -2 * (4 - (x**4 + x**2)) * (4 * x**3 + 2 * x) + x / 2\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate=0.001, max_iter=10000, tol=1e-6):\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x -= learning_rate * grad\n",
    "    return x\n",
    "\n",
    "x0 = 0 # initial guess\n",
    "m = gradient_descent(f, grad_f, x0)\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095842e",
   "metadata": {},
   "source": [
    "## Solution 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb34700",
   "metadata": {},
   "source": [
    "\n",
    "In this case, we have:\n",
    "\n",
    "$$\n",
    "V(x) = -\\log(\\pi_{X \\mid Y}(x \\mid Y=4)) = -\\log(e^{-\\frac{(4-(x^4+x^2))^2}{2}}e^{-\\frac{x^2}{4}}) = \\frac{(4-(x^4+x^2))^2}{2} + \\frac{x^2}{4}\n",
    "$$\n",
    "\n",
    "Taking the second derivative with respect to $x$, we get:\n",
    "\n",
    "$$\n",
    "V''(x) = \\frac{d^2}{dx^2}V(x) = 12x^4 + 48x^2 - 38\n",
    "$$\n",
    "\n",
    "Substituting the value of $m$ that we found in the previous step (which was 0), we get:\n",
    "\n",
    "$$\n",
    "V''(m) = V''(0) = -38\n",
    "$$\n",
    "\n",
    "Therefore, the Laplace approximation to the posterior density of $X$ given $Y=4$ is:\n",
    "\n",
    "$$\n",
    "\\tilde{\\pi}(x) = N\\left(m, V''(m)^{-1}\\right) = N\\left(0, \\frac{1}{38}\\right)\n",
    "$$\n",
    "\n",
    "This means that the Laplace approximation to the posterior density of $X$ given $Y=4$ is a normal distribution with mean 0 and variance $\\frac{1}{38}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d9e52",
   "metadata": {},
   "source": [
    "## Solution 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327ce656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return (4 - (x**4 + x**2))**2 / 2 + x**2 / 4\n",
    "\n",
    "def grad_f(x):\n",
    "    return -2 * (4 - (x**4 + x**2)) * (4 * x**3 + 2 * x) + x / 2\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate=0.001, max_iter=10000, tol=1e-6):\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x -= learning_rate * grad\n",
    "    return x\n",
    "\n",
    "def grad_V(x):\n",
    "    return -2 * (4 - (x**4 + x**2)) * (4 * x**3 + 2 * x) + x / 2\n",
    "\n",
    "def langevin(grad_V, x0, dt, T):\n",
    "    N = int(T / dt)\n",
    "    X = np.zeros(N+1)\n",
    "    X[0] = x0\n",
    "    for i in range(N):\n",
    "        X[i+1] = X[i] - grad_V(X[i]) * dt + np.sqrt(2*dt) * np.random.randn()\n",
    "    return X\n",
    "\n",
    "x0 = gradient_descent(f, grad_f, 0) # initial value\n",
    "dt = 0.001 # time step\n",
    "T = 100 # terminal time\n",
    "X = langevin(grad_V, x0, dt, T)\n",
    "\n",
    "# plot histogram of samples\n",
    "plt.hist(X, bins=50, density=True)\n",
    "\n",
    "# plot Laplace approximation\n",
    "x = np.linspace(-1.5, 1.5, 100)\n",
    "mu = 0 # mean of Laplace approximation\n",
    "sigma = np.sqrt(1/38) # standard deviation of Laplace approximation\n",
    "y = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a552e50",
   "metadata": {},
   "source": [
    "## Solution 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d91d50",
   "metadata": {},
   "source": [
    "The two methods you mentioned are the Laplace approximation and the Langevin method. Both methods can be used to approximate the posterior density of $X$ given $Y=y$, but they have different strengths and weaknesses.\n",
    "\n",
    "The Laplace approximation is a fast and simple method for approximating a probability density function using a normal distribution. It only requires computing the maximum a posteriori (MAP) estimate of $X$ given $Y=y$ and the second derivative of the negative log of the target density at this point. However, the accuracy of the Laplace approximation depends on how well the target density can be approximated by a normal distribution. If the target density is highly non-normal or has multiple modes, the Laplace approximation may not be very accurate.\n",
    "\n",
    "The Langevin method is a Markov Chain Monte Carlo (MCMC) algorithm that can be used to sample from a target distribution. It generates a sequence of samples from the target distribution by simulating a Markov chain that has the target distribution as its invariant distribution. The Langevin method can be more accurate than the Laplace approximation because it can handle target densities that are highly non-normal or have multiple modes. However, it requires more computational effort because it involves simulating a Markov chain for a sufficiently long time until convergence to the invariant distribution. The accuracy of the Langevin method also depends on the choice of parameters such as the time step and initial value.\n",
    "\n",
    "In summary, the Laplace approximation is a fast and simple method for approximating a probability density function using a normal distribution, but its accuracy may be limited if the target density is highly non-normal or has multiple modes. The Langevin method is a more accurate but computationally more expensive method that can handle more complex target densities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce01e8",
   "metadata": {},
   "source": [
    "## Problem 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90b650",
   "metadata": {},
   "source": [
    "The joint distribution of $X$ and $Y$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}X\\\\Y\\end{pmatrix} = \\begin{pmatrix}X_1\\\\X_2\\\\X_1+X_2+W\\end{pmatrix} \\sim N\\left(\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}, \\begin{pmatrix}1&0&1\\\\0&1&1\\\\1&1&2+\\sigma^2\\end{pmatrix}\\right)\n",
    "$$\n",
    "\n",
    "Using the formula for the conditional distribution of a multivariate normal distribution, we have:\n",
    "\n",
    "$$\n",
    "X|Y=y \\sim N\\left(\\mu_{X|Y=y}, \\Sigma_{X|Y=y}\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mu_{X|Y=y} = \\mu_X + \\Sigma_{XY}\\Sigma_{Y}^{-1}(y-\\mu_Y)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Sigma_{X|Y=y} = \\Sigma_X - \\Sigma_{XY}\\Sigma_Y^{-1}\\Sigma_{YX}\n",
    "$$\n",
    "\n",
    "Substituting the values for the mean vector and covariance matrix of the joint distribution of X and Y, we get:\n",
    "\n",
    "$$\n",
    "\\mu_{X|Y=y} = \\begin{pmatrix}0\\\\0\\end{pmatrix} + \\begin{pmatrix}1&0\\\\1&1\\end{pmatrix}\\begin{pmatrix}2+\\sigma^2\\end{pmatrix}^{-1}(y-0) = \\frac{y}{2+\\sigma^2}\\begin{pmatrix}1\\\\1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Sigma_{X|Y=y} = \\begin{pmatrix}1&0\\\\0&1\\end{pmatrix} - \\begin{pmatrix}1&0\\\\1&1\\end{pmatrix}\\begin{pmatrix}2+\\sigma^2\\end{pmatrix}^{-1}\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix} = \\frac{\\sigma^2}{2+\\sigma^2}\\begin{pmatrix}1&-1\\\\-1&1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, the conditional distribution of $X$ given $Y = y$ is a bivariate normal distribution with mean vector $(y/(2+ÏƒÂ²)$, $y/(2+ÏƒÂ²))$ and covariance matrix $(ÏƒÂ²/(2+ÏƒÂ²))((1,-1),(-1, 1))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92795401",
   "metadata": {},
   "source": [
    "## Problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8b069",
   "metadata": {},
   "source": [
    "As $Ïƒ â†’ 0$, the variance of the noise $W$ goes to 0, which means that the noise has less and less effect on the value of $Y$. In this case, the mean vector of the conditional distribution of $X1$ and $X2$ given $Y = y$ approaches:\n",
    "\n",
    "$$\n",
    "E\\begin{pmatrix}X_1\\\\X_2\\end{pmatrix}\\Big|Y=y = \\begin{pmatrix}\\frac{y}{2}\\\\\\frac{y}{2}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and the covariance matrix approaches:\n",
    "\n",
    "$$\n",
    "Cov\\begin{pmatrix}X_1\\\\X_2\\end{pmatrix}\\Big|Y=y = \\begin{pmatrix}0&0\\\\0&0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This means that, given $Y = y$, $X1$ and $X2$ become perfectly negatively correlated and approach the value $y/2$ with probability 1.\n",
    "\n",
    "On the other hand, as $Ïƒ â†’ \\infty$, the variance of the noise $W$ goes to infinity, which means that the noise has an increasingly larger effect on the value of $Y$. In this case, the mean vector of the conditional distribution of $X1$ and $X2$ given $Y = y$ approaches:\n",
    "\n",
    "$$\n",
    "E\\begin{pmatrix}X_1\\\\X_2\\end{pmatrix}\\Big|Y=y = \\begin{pmatrix}0\\\\0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and the covariance matrix approaches:\n",
    "\n",
    "$$\n",
    "Cov\\begin{pmatrix}X_1\\\\X_2\\end{pmatrix}\\Big|Y=y = \\begin{pmatrix}\\frac{1}{2}&0\\\\0&\\frac{1}{2}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This means that, given $Y = y$, $X1$ and $X2$ become independent and their conditional distribution approaches $N(0, 1/2)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
